{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will scrap data like post, comments from reddit and make a dataset will be lated used for Natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.selenium_scraper import SeleniumScraper\n",
    "from core.soup_scraper import SoupScraper\n",
    "from core.progress_bar import ProgressBar\n",
    "from core.sql_access import SqlAccess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in g:\\anaconda\\lib\\site-packages (from selenium) (1.25.8)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_home = 'https://www.reddit.com'\n",
    "slash = '/r/'\n",
    "subreddit = 'DataScience'\n",
    "sort_by = '/hot/'\n",
    "scroll_n_times = 1000\n",
    "scrape_comments = True\n",
    "erase_db_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = SqlAccess()\n",
    "SelScraper = SeleniumScraper()\n",
    "BSS = SoupScraper(reddit_home,\n",
    "                  slash,\n",
    "                  subreddit)\n",
    "\n",
    "SelScraper.setup_chrome_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening reddit and scrolling: takes approximately 500.0 seconds\n",
      "Collected 0 links\n"
     ]
    }
   ],
   "source": [
    "# Collect links from subreddit\n",
    "links = SelScraper.collect_links(page = reddit_home + \n",
    "                                        slash + subreddit + sort_by,\n",
    "                                 scroll_n_times = scroll_n_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding <script id=\"data\"> for each link\n"
     ]
    }
   ],
   "source": [
    "# Find the <script> with id='data' for each link\n",
    "script_data = BSS.get_scripts(urls = links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Python dicts out of script data\n"
     ]
    }
   ],
   "source": [
    "# Transforms each script with data into a Python dict, returned as [{}, {}...]\n",
    "BSS.data = SelScraper.reddit_data_to_dict(script_data = script_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data...\n"
     ]
    }
   ],
   "source": [
    "print('Scraping data...')\n",
    "progress = ProgressBar(len(links))\n",
    "for i, current_data in enumerate(BSS.data):\n",
    "    progress.update()\n",
    "    \n",
    "    BSS.get_url_id_and_url_title(BSS.urls[i],\n",
    "                                 current_data, i)\n",
    "    BSS.get_title()\n",
    "    BSS.get_upvote_ratio()\n",
    "    BSS.get_score()\n",
    "    BSS.get_posted_time()\n",
    "    BSS.get_author()\n",
    "    BSS.get_flairs()\n",
    "    BSS.get_num_gold()\n",
    "    BSS.get_category()\n",
    "    BSS.get_total_num_comments()\n",
    "    BSS.get_links_from_post()\n",
    "    BSS.get_main_link()\n",
    "    BSS.get_text()\n",
    "    BSS.get_comment_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data...\n",
      "Gathering all the scraped data, and scraping ALL comment data (very slow, dependent on number of comments)\n",
      "\n",
      "It took 2.0 seconds to scrape and store 0 links\n"
     ]
    }
   ],
   "source": [
    "print('Scraping data...')\n",
    "start = time.time()\n",
    "progress = ProgressBar(len(links))\n",
    "for i, current_data in enumerate(BSS.data):\n",
    "    progress.update()\n",
    "    \n",
    "    BSS.get_url_id_and_url_title(BSS.urls[i],\n",
    "                                 current_data, i)\n",
    "    BSS.get_title()\n",
    "    BSS.get_upvote_ratio()\n",
    "    BSS.get_score()\n",
    "    BSS.get_posted_time()\n",
    "    BSS.get_author()\n",
    "    BSS.get_flairs()\n",
    "    BSS.get_num_gold()\n",
    "    BSS.get_category()\n",
    "    BSS.get_total_num_comments()\n",
    "    BSS.get_links_from_post()\n",
    "    BSS.get_main_link()\n",
    "    BSS.get_text()\n",
    "    BSS.get_comment_ids()\n",
    "\n",
    "time.sleep(1)\n",
    "BSS.prepare_data_for_sql(scrape_comments=scrape_comments)\n",
    "\n",
    "try:\n",
    "    SQL.create_or_connect_db(erase_first=erase_db_first)\n",
    "    # [0] = post, [1] = comment, [2] = link\n",
    "    for i in range(len(BSS.post_data)):\n",
    "        SQL.insert('post', data = BSS.post_data[i])\n",
    "        SQL.insert('link', data = BSS.link_data[i])\n",
    "        \n",
    "        if scrape_comments:\n",
    "            SQL.insert('comment', data = BSS.comment_data[i])\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "finally:\n",
    "    SQL.save_changes()\n",
    "\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "\n",
    "print(('\\nIt took {0} seconds to scrape and store {1} links').format(round(end - start, 1),\n",
    "                                                                     len(links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset is saved into sql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to enter new data to database\n",
    "try:\n",
    "    SQL.create_or_connect_db(erase_first=erase_db_first)\n",
    "    # [0] = post, [1] = comment, [2] = link\n",
    "    for i in range(len(BSS.post_data)):\n",
    "        SQL.insert('post', data = BSS.post_data[i])\n",
    "        SQL.insert('link', data = BSS.link_data[i])\n",
    "        \n",
    "        if scrape_comments:\n",
    "            SQL.insert('comment', data = BSS.comment_data[i])\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "finally:\n",
    "    SQL.save_changes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets make dataset\n",
    "from core.sql_access import SqlAccess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to database\n",
    "SQL = SqlAccess()\n",
    "SQL.create_or_connect_db()\n",
    "c = SQL.conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve data from database table\n",
    "all_data = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM post p \n",
    "LEFT JOIN comment c \n",
    "    ON p.id = c.post_id\n",
    "LEFT JOIN link l\n",
    "\tON p.id = l.post_id;\n",
    "\"\"\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect posts and comments data\n",
    "post = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM post;\n",
    "\"\"\", c)\n",
    "\n",
    "comment = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM comment;\n",
    "\"\"\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "all_data.to_csv('data/post_comment_link_data_demo.csv', columns=all_data.columns, index=False)\n",
    "post.to_csv('data/post_data_demo.csv', columns=post.columns, index=False)\n",
    "comment.to_csv('data/comment_data_demo.csv', columns=comment.columns, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
